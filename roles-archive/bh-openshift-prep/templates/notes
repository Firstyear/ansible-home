
For the ldap auth you need:

/etc/origin/master/master-config.yaml

oauthConfig:
  assetPublicURL: https://osmaster.dev.blackhats.net.au:8443/console/
  grantConfig:
    method: auto
  #identityProviders:
  #- challenge: true
  #  login: true
  #  mappingMethod: claim
  #  name: allow_all
  #  provider:
  #    apiVersion: v1
  #    kind: AllowAllPasswordIdentityProvider
  identityProviders:
  - challenge: true
    login: true
    mappingMethod: claim
    name: bh_ldap
    provider:
     apiVersion: v1
     kind: LDAPPasswordIdentityProvider
     attributes:
      id:
      - dn
      email:
      - mail
      name:
      - cn
      preferredUsername:
      - uid
     ca: /etc/pki/tls/certs/bh_ldap.crt
     insecure: false
     url: "ldap://ldap.blackhats.net.au/ou=People,dc=blackhats,dc=net,dc=au?uid?sub?(memberOf=cn=bh_admins,ou=Groups,dc=blackhats,dc=net,dc=au)"
  masterCA: ca-bundle.crt
  masterPublicURL: https://osmaster.dev.blackhats.net.au:8443
  masterURL: https://osmaster.dev.blackhats.net.au:8443
  sessionConfig:
    sessionMaxAgeSeconds: 3600
    sessionName: ssn
    sessionSecretsFile: /etc/origin/master/session-secrets.yaml
  tokenConfig:
    accessTokenMaxAgeSeconds: 86400
    authorizeTokenMaxAgeSeconds: 500


You will need to update ip6tables too:

-A INPUT -p tcp -m state --state NEW -m tcp --dport 10080 -s 2001:44b8:2155:2c11:5054:ff:feba:574 -j ACCEPT


Reboot the nodes at this point.

If you see the registry and router fail to deploy, run:

oc deploy --cancel dc/router -n default
oc latest --cancel dc/router -n default


corsAllowedOrigins:
  - 127.0.0.1
  - localhost
  - 172.24.11.19
  - kubernetes.default
  - kubernetes.default.svc.cluster.local
  - kubernetes
  - openshift.default
  - osmaster.dev.blackhats.net.au
  - openshift
  - openshift.default.svc
  - 172.30.0.1
  - openshift.default.svc.cluster.local
  - kubernetes.default.svc
  - openshift.dev.blackhats.net.au      <<-- add this

Create persistent storage next:

Make iscsi volume on the storage server

targetcli /backstores/fileio create os-dev-t1-pv-1 `pwd`/os-dev-t1-pv-1 10G write_back=false
targetcli /iscsi/iqn.2003-01.org.linux-iscsi.mion.x8664:sn.96c325adf995/tpg1/luns/ create /backstores/fileio/os-dev-t1-pv-1

On osmaster:

iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.mion.x8664:sn.96c325adf995 -l

for i in {b,c,d,e,f,g,h,i,j,k}; do mkfs.xfs /dev/sd$i; done

iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.mion.x8664:sn.96c325adf995 --logout

Now import the volumes with:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: os-dev-t1-pv-XXX
spec:
  capacity:
    storage: 8Gi
  accessModes:
    - ReadWriteOnce
  iscsi:
     targetPortal: 172.24.10.8
     iqn: iqn.2003-01.org.linux-iscsi.mion.x8664:sn.96c325adf995
     lun: XXX
     fsType: 'xfs'
     readOnly: false



Make the registry use a persistent volume.

Create a backing store of sufficent capacity.

targetcli /backstores/fileio create os-dev-t2-pv-11 `pwd`/os-dev-t2-pv-11 80G write_back=false
targetcli /iscsi/iqn.2003-01.org.linux-iscsi.mion.x8664:sn.96c325adf995/tpg1/luns/ create /backstores/fileio/os-dev-t2-pv-11

iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.mion.x8664:sn.96c325adf995 -l
mkfs.xfs /dev/sdXXXX
iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.mion.x8664:sn.96c325adf995 --logout

Now make the volume

apiVersion: v1
kind: PersistentVolume
metadata:
  name: os-dev-t2-pv-11
spec:
  capacity:
    storage: 78Gi
  accessModes:
    - ReadWriteOnce
  iscsi:
     targetPortal: 172.24.10.8
     iqn: iqn.2003-01.org.linux-iscsi.mion.x8664:sn.96c325adf995
     lun: 0
     fsType: 'xfs'
     readOnly: false

Create a claim for it



apiVersion: "v1"
kind: "PersistentVolumeClaim"
metadata:
  name: "claim-os-dev-t2-pv-11"
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "78Gi"
  volumeName: "os-dev-t2-pv-11"


Now attach this to the registry for usage.

oc volume deploymentconfigs/docker-registry --add --name=registry-storage -t pvc --claim-name=claim-os-dev-t2-pv-11 --overwrite



Can push images with:

sudo docker login -u william -p `oc whoami -t` docker-registry-default.openshift.dev.blackhats.net.au:80
sudo docker tag centos_wibrown_devel:7 docker-registry-default.openshift.dev.blackhats.net.au:80/centos7sh/centos_wibrown_devel
sudo docker push docker-registry-default.openshift.dev.blackhats.net.au:80/centos7sh/centos_wibrown_devel

oc run -i --tty zsh --image=centos_wibrown_devel --restart=Never

##### Change a default SCC for a project!!!!

# Allows run as root!
# You can always copy the SCC to change the user run as etc
oadm policy add-scc-to-user anyuid -z default




### Running container as root:

You need to use the anyuid or privileged policy:

if you specify the in the dockefile "USER root" it will IGNORE you and just map to some crazy uid anyway.

# [root@osmaster]/home/william# oadm policy add-scc-to-user privileged system:serviceaccount:centos7ssh:default

[root@osmaster]/home/william# oadm policy add-scc-to-user anyuid system:serviceaccount:centos7ssh:default
[root@osmaster]/home/william# oc policy add-role-to-user admin william -n centos7ssh


### How do you make the load balancer use your own domain name.


### Docker won't start and everything explodes. 

Diagnosing:

[root@osmaster]/var/lib/docker/network# oc get nodes
NAME                            STATUS     AGE
osmaster.dev.blackhats.net.au   Ready      91d
osnode01.dev.blackhats.net.au   Ready      91d
osnode02.dev.blackhats.net.au   NotReady   91d

Now describe the node

[root@osmaster]/var/lib/docker/network# oc describe node osnode02.dev.blackhats.net.au
Name:           osnode02.dev.blackhats.net.au
Labels:         kubernetes.io/hostname=osnode02.dev.blackhats.net.au
            region=primary
            zone=west
Taints:         <none>
CreationTimestamp:  Thu, 25 Aug 2016 09:37:28 +1000
Phase:          
Conditions:
  Type          Status      LastHeartbeatTime           LastTransitionTime          Reason              Message
  ----          ------      -----------------           ------------------          ------              -------
  OutOfDisk         Unknown     Sat, 29 Oct 2016 06:22:08 +1000     Sat, 29 Oct 2016 06:22:49 +1000     NodeStatusUnknown       Kubelet stopped posting node status.
  Ready         Unknown     Sat, 29 Oct 2016 06:22:08 +1000     Sat, 29 Oct 2016 06:22:49 +1000     NodeStatusUnknown       Kubelet stopped posting node status.
  MemoryPressure    False       Sat, 29 Oct 2016 06:22:08 +1000     Sat, 01 Oct 2016 06:19:33 +1000     KubeletHasSufficientMemory  kubelet has sufficient memory available
Addresses:      172.24.11.23,172.24.11.23
Capacity:
 alpha.kubernetes.io/nvidia-gpu:    0
 cpu:                   2
 memory:                5946808Ki
 pods:                  110
Allocatable:
 alpha.kubernetes.io/nvidia-gpu:    0
 cpu:                   2
 memory:                5946808Ki
 pods:                  110
System Info:
 Machine ID:            46f5c48af52e4d05b9bbc0539fa94ac0
 System UUID:           46F5C48A-F52E-4D05-B9BB-C0539FA94AC0
 Boot ID:           a36c3479-23af-451d-bc33-5710fc1e733d
 Kernel Version:        3.10.0-327.36.2.el7.x86_64
 OS Image:          CentOS Linux 7 (Core)
 Operating System:      linux
 Architecture:          amd64
 Container Runtime Version: docker://1.10.3
 Kubelet Version:       v1.3.0+52492b4
 Kube-Proxy Version:        v1.3.0+52492b4
ExternalID:         osnode02.dev.blackhats.net.au
Non-terminated Pods:        (0 in total)
  Namespace         Name        CPU Requests    CPU Limits  Memory Requests Memory Limits
  ---------         ----        ------------    ----------  --------------- -------------
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted. More info: http://releases.k8s.io/HEAD/docs/user-guide/compute-resources.md)
  CPU Requests  CPU Limits  Memory Requests Memory Limits
  ------------  ----------  --------------- -------------
  0 (0%)    0 (0%)      0 (0%)      0 (0%)
Events:
  FirstSeen LastSeen    Count   From            SubobjectPath   Type        Reason      Message
  --------- --------    -----   ----            -------------   --------    ------      -------
  29m       29m     1   {controllermanager }            Normal      RegisteredNode  Node osnode02.dev.blackhats.net.au event: Registered Node osnode02.dev.blackhats.net.au in NodeController



The kubelet is origin-node service, so check it.

[root@osnode02]/home/william# systemctl status origin-node
● origin-node.service - Origin Node
   Loaded: loaded (/usr/lib/systemd/system/origin-node.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/origin-node.service.d
           └─openshift-sdn-ovs.conf
   Active: inactive (dead) (Result: exit-code) since Thu 2016-11-24 13:48:21 AEST; 347ms ago
     Docs: https://github.com/openshift/origin
  Process: 51163 ExecStart=/usr/bin/openshift start node --config=${CONFIG_FILE} $OPTIONS (code=exited, status=255)
 Main PID: 51163 (code=exited, status=255)
   Memory: 0B
   CGroup: /system.slice/origin-node.service

Nov 24 13:48:16 osnode02.dev.blackhats.net.au systemd[1]: origin-node.service: main process exited, code=exited, status=255/n/a
Nov 24 13:48:16 osnode02.dev.blackhats.net.au systemd[1]: Failed to start Origin Node.
Nov 24 13:48:16 osnode02.dev.blackhats.net.au systemd[1]: Unit origin-node.service entered failed state.
Nov 24 13:48:16 osnode02.dev.blackhats.net.au systemd[1]: origin-node.service failed.
Nov 24 13:48:21 osnode02.dev.blackhats.net.au systemd[1]: origin-node.service holdoff time over, scheduling restart.


And it's failing because:

[root@osnode02]/home/william# systemctl status docker
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/docker.service.d
           └─docker-sdn-ovs.conf
   Active: failed (Result: exit-code) since Thu 2016-11-24 13:48:46 AEST; 2s ago
     Docs: http://docs.docker.com
  Process: 51427 ExecStart=/usr/bin/docker-current daemon --exec-opt native.cgroupdriver=systemd $OPTIONS $DOCKER_STORAGE_OPTIONS $DOCKER_NETWORK_OPTIONS $ADD_REGISTRY $BLOCK_REGISTRY $INSECURE_REGISTRY (code=exited, status=1/FAILURE)
 Main PID: 51427 (code=exited, status=1/FAILURE)

Nov 24 13:48:42 osnode02.dev.blackhats.net.au systemd[1]: Starting Docker Application Container Engine...
Nov 24 13:48:42 osnode02.dev.blackhats.net.au docker-current[51427]: time="2016-11-24T13:48:42.660143462+10:00" level=info msg="Graph migration to content-addressability took 0.00 seconds"
Nov 24 13:48:42 osnode02.dev.blackhats.net.au docker-current[51427]: time="2016-11-24T13:48:42.665369044+10:00" level=info msg="Firewalld running: false"
Nov 24 13:48:46 osnode02.dev.blackhats.net.au docker-current[51427]: time="2016-11-24T13:48:46.460166613+10:00" level=fatal msg="Error starting daemon: Error initializing network controller: could...e endpoints"
Nov 24 13:48:46 osnode02.dev.blackhats.net.au systemd[1]: docker.service: main process exited, code=exited, status=1/FAILURE
Nov 24 13:48:46 osnode02.dev.blackhats.net.au systemd[1]: Failed to start Docker Application Container Engine.
Nov 24 13:48:46 osnode02.dev.blackhats.net.au systemd[1]: Unit docker.service entered failed state.
Nov 24 13:48:46 osnode02.dev.blackhats.net.au systemd[1]: docker.service failed.
Hint: Some lines were ellipsized, use -l to show in full.



The key is "Error initializing network controller: could". Solution is in https://github.com/docker/docker/issues/18283.

[root@osnode02]/home/william# cd /var/lib/docker/network 
[root@osnode02]/var/lib/docker/network# mv files files-backup

Fixed.

### Deploying squid

Build and push the container.

sudo docker login -u william -p `oc whoami -t` docker-registry-default.openshift.dev.blackhats.net.au:80
sudo docker tag centos_squid:7 docker-registry-default.openshift.dev.blackhats.net.au:80/squid-demo/centos_squid:7
sudo docker push docker-registry-default.openshift.dev.blackhats.net.au:80/squid-demo/centos_squid:7

First make the new app on the oc cluster

oc project <name>
oc new-app centos_squid:7

Attach storage to the deployment configuration.

oc volume dc/centossquid --add --name=centossquid-volume-1 -t pvc --claim-name=centossquid-volume-1 --overwrite

# Now debug in a terminal to run the FS fixes



### Give a server an external IP

deploy it


Then you can map with yml like

apiVersion: v1
kind: Service
metadata:
  name: squid-1
spec:
  ports:
  - name: squid
    port: 3128
  type: LoadBalancer
  selector:
    name: squid-selector


Key is to match the name to the 



### If the registry is redeployed, you should restart the master else it MAY fuck shit up

