# Safe defaults

# Kernel numa balancing moves tasks / memory to stay within a numa region
# This HUGELY improves performance.
# Does nothing on machines with a single numa region.
kernel.numa_balancing = 1

# The total time the scheduler will consider a migrated process
# # "cache hot" and thus less likely to be re-migrated
# # (system default is 500000, i.e. 0.5 ms)
kernel.sched_migration_cost_ns = 5000000

# On workstations, processes are all started on the same core. This means they
# Hit a single cpu hard. This disables that grouping and starts processes 
# migrating
kernel.sched_autogroup_enabled = 0

# If a workload mostly uses anonymous memory and it hits this limit, the entire
# working set is buffered for I/O, and any more write buffering would require
# swapping, so it's time to throttle writes until I/O can catch up.  Workloads
# that mostly use file mappings may be able to use even higher values.
#
# The generator of dirty data starts writeback at this percentage (system default
# is 20%)
vm.dirty_ratio = 30

# Start background writeback (via writeback threads) at this percentage (system
# default is 10%)
# Start getting data out to disk as soon as we can.
vm.dirty_background_ratio = 5

# We want most objects in the cache to be elegible for writeout. 100ths of a second.
# Ie in this case it must have been in memory for 3 seconds before we consider writing it
# default 3000
vm.dirty_expire_centisecs = 300

# How often should we wake the flushing threads? Measured in 100ths of a second.
# default 500
vm.dirty_writeback_centisecs = 50

# Slightly prefer to keep inode information around. Will really help on large workloads.
# default 100
vm.vfs_cache_pressure = 80

# default 50
# vm.overcommit_ratio = 50

# The swappiness parameter controls the tendency of the kernel to move
# processes out of physical memory and onto the swap disk.
# 0 tells the kernel to avoid swapping processes out of physical memory
# for as long as possible
# 100 tells the kernel to aggressively swap processes out of physical memory
# and move them to swap cache
vm.swappiness=10

#### If you want to use these, you should copy them out to another file, or talk to SSS

# ktune sysctl settings for rhel6 servers, maximizing i/o throughput
#
# Minimal preemption granularity for CPU-bound tasks:
# (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
#kernel.sched_min_granularity_ns = 10000000

# SCHED_OTHER wake-up granularity.
# (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
#
# This option delays the preemption effects of decoupled workloads
# and reduces their over-scheduling. Synchronous workloads will still
# have immediate wakeup/sleep latencies.
#kernel.sched_wakeup_granularity_ns = 15000000

# PID allocation wrap value.  When the kernel's next PID value
# reaches this value, it wraps back to a minimum PID value.
# PIDs of value pid_max or larger are not allocated.
#
# A suggested value for pid_max is 1024 * <# of cpu cores/threads in system>
# e.g., a box with 32 cpus, the default of 32768 is reasonable, for 64 cpus,
# 65536, for 4096 cpus, 4194304 (which is the upper limit possible).
#kernel.pid_max = 65536


# This is pretty safe, could be a default.
# IP
# We should consider setting this to 0, so that resumed connections are "snappier".
# Changes tcp window behavour to not reset window size after idle
# net.ipv4.tcp_slow_start_after_idle = 1
#
#

# These allow the application socket to directly negotiate with the driver. Can
# provide huge tcp latency benefits (reductions) at the expense of wake ups
# https://www.kernel.org/doc/Documentation/sysctl/net.txt
# net.core.busy_read=50
# net.core.busy_poll=50 


# Allows sending data in the first SYN packet from connections we initiate and receive
# Setting to 3 allows us to send and recieve this.
# https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt
# net.ipv4.tcp_fastopen=3

