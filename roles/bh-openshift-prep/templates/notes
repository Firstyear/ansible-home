
For the ldap auth you need:

/etc/origin/master/master-config.yaml

oauthConfig:
  assetPublicURL: https://osmaster.dev.blackhats.net.au:8443/console/
  grantConfig:
    method: auto
  #identityProviders:
  #- challenge: true
  #  login: true
  #  mappingMethod: claim
  #  name: allow_all
  #  provider:
  #    apiVersion: v1
  #    kind: AllowAllPasswordIdentityProvider
  identityProviders:
  - challenge: true
    login: true
    mappingMethod: claim
    name: bh_ldap
    provider:
     apiVersion: v1
     kind: LDAPPasswordIdentityProvider
     attributes:
      id:
      - dn
      email:
      - mail
      name:
      - cn
      preferredUsername:
      - uid
     ca: /etc/pki/tls/certs/bh_ldap.crt
     insecure: false
     url: "ldap://ldap01.blackhats.net.au/ou=People,dc=blackhats,dc=net,dc=au?uid?sub?(memberOf=cn=bh_admins,ou=Groups,dc=blackhats,dc=net,dc=au)"
  masterCA: ca-bundle.crt
  masterPublicURL: https://osmaster.dev.blackhats.net.au:8443
  masterURL: https://osmaster.dev.blackhats.net.au:8443
  sessionConfig:
    sessionMaxAgeSeconds: 3600
    sessionName: ssn
    sessionSecretsFile: /etc/origin/master/session-secrets.yaml
  tokenConfig:
    accessTokenMaxAgeSeconds: 86400
    authorizeTokenMaxAgeSeconds: 500


You will need to update ip6tables too:

-A INPUT -p tcp -m state --state NEW -m tcp --dport 10080 -s 2001:44b8:2155:2c11:5054:ff:feba:574 -j ACCEPT


Reboot the nodes at this point.

If you see the registry and router fail to deploy, run:

oc deploy --cancel dc/router -n default
oc latest --cancel dc/router -n default


corsAllowedOrigins:
  - 127.0.0.1
  - localhost
  - 172.24.11.19
  - kubernetes.default
  - kubernetes.default.svc.cluster.local
  - kubernetes
  - openshift.default
  - osmaster.dev.blackhats.net.au
  - openshift
  - openshift.default.svc
  - 172.30.0.1
  - openshift.default.svc.cluster.local
  - kubernetes.default.svc
  - openshift.dev.blackhats.net.au      <<-- add this

Create persistent storage next:

https://docs.openshift.org/latest/install_config/persistent_storage/persistent_storage_nfs.html

Make the volume accesible:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: os-dev-vol-1
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: /var/lib/openshift/os-dev-vol-1
    server: lorna.prd.blackhats.net.au
  persistentVolumeReclaimPolicy: Recycle

oc create -f <file.yml>

You will need to use this claim template to test the volume:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-os-dev-vol-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


To use the volumes, you must make a sec policy. See: https://docs.openshift.org/latest/install_config/persistent_storage/pod_security_context.html


oc export scc restricted > new-scc.yaml

[root@osmaster]~# cat new-scc.yaml 
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegedContainer: false
allowedCapabilities: null
apiVersion: v1
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
groups:
- system:authenticated
kind: SecurityContextConstraints
metadata:
  annotations:
    kubernetes.io/description: restricted denies access to all host features and requires
      pods to be run with a UID, and SELinux context that are allocated to the namespace.  This
      is the most restrictive SCC.
  creationTimestamp: null
  name: restricted
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
- KILL
- MKNOD
- SYS_CHROOT
- SETUID
- SETGID
runAsUser:
  type: MustRunAsRange
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: RunAsAny
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- secret

Edit it to make the nfs-scc.

[root@osmaster]~# cat nfs-scc.yml 
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegedContainer: false
allowedCapabilities: null
apiVersion: v1
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
groups:
- system:authenticated
kind: SecurityContextConstraints
metadata:
  annotations:
    kubernetes.io/description: nfs denies access to all host features and requires
      pods to be run with a UID, and SELinux context that are allocated to the namespace.  This
      is the most restrictive SCC. Allows access to NFS as nfsnobody.
  creationTimestamp: null
  name: nfs-scc
priority: 9
readOnlyRootFilesystem: false
requiredDropCapabilities:
- KILL
- MKNOD
- SYS_CHROOT
- SETUID
- SETGID
runAsUser:
  type: MustRunAsRange
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: MustRunAs
  ranges:
  - min: 65534
    max: 65534
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- secret

I don't know if this is secure, because all contains will have access to all the other nfs claims .....

Create with:

[root@osmaster]~# oc create -f nfs-scc.yml 
securitycontextconstraints "nfs-scc" created


Look at the project default

oc get project default -o yaml

Now edit the pod to apply

[root@osmaster]~# oc get pod
NAME                  READY     STATUS      RESTARTS   AGE
insults-1-build       0/1       Completed   0          1h
insults-2-8gsym       1/1       Running     0          49m
postgresql-1-deploy   0/1       Error       0          55m

oc edit pod postgresql-1-deploy

metadata:
  annotations:
    openshift.io/deployment.name: postgresql-1
    openshift.io/scc: nfs-scc





